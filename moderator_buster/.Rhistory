#Custom Functions#
##################
source("ML_Custom_Functions.R")
######
#Data#
######
#0. Recoding key for test cases
recode_key <- read.csv("Medsinc - Replace Missing Test - Sheet1.csv", header = FALSE, stringsAsFactors = FALSE) #the warning message about incomplete final line is ok here
#1. Training data
train <- list()
train[["10k"]] <- read.csv("Training Data/medsinc-results-10k.csv")
#train[["100k"]] <- read.csv("Training Data/medsinc-results-100k.csv")
#2. Testing data
raw_test <- read.csv("Testing Data/Medsinc Testing for Andy - Combined Sheet.csv", na.strings = c("-", "", " "))
#making values numeric (you should see a warning about NAs being introduced by coercion)
raw_test$questions.weight_in_kg <-  as.numeric(as.character(raw_test$questions.weight_in_kg))
raw_test$questions.temperature_in_celcius <- as.numeric(as.character(raw_test$questions.temperature_in_celcius))
raw_test$questions.oxygen_staturation_in_percent <- as.numeric(as.character(raw_test$questions.oxygen_staturation_in_percent))
#Replacing error in questions.diarrhea
raw_test[,"questions.diarrhea"] <- as.character(raw_test[,"questions.diarrhea"])
replace_questions.diarrhea_yes <- which(raw_test[,"questions.diarrhea"] == "yes")
if(length(replace_questions.diarrhea_yes) > 0){
raw_test[replace_questions.diarrhea_yes,"questions.diarrhea"] <- NA
}
raw_test[,"questions.diarrhea"] <- as.factor(raw_test[,"questions.diarrhea"])
#Translating testing data (there are missing data here, which need to be re-coded to match the training data)
test_recoded <- recode_test_data(test_data = raw_test, cols_to_recode = recode_key[1,], replace_missing = recode_key[2,]) #shouldn't need to change any of the defaults
#There should be no errors or warnings from the above function
#3. Building data sets for model building and selecting the variable for testing.
#get the question columns
training_data <- train[[training_set]]
question_cols <- grep("questions", colnames(training_data), ignore.case = TRUE)
train_model_data <- training_data[,question_cols]
test_questions_cols <- which(colnames(test_recoded) %in% colnames(train_model_data))
test_recoded_model_data <- test_recoded[,test_questions_cols]
#checking to make sure all the columns in training are in testing.
if(length(which(!colnames(train_model_data) %in% colnames(test_recoded_model_data))) != 0){
stop("We're missing some columns in the testing data")
}
#checking to make sure there are no factor problems
for(i in 1:ncol(train_model_data)){
mt.i <- which(colnames(test_recoded_model_data) == colnames(train_model_data)[i])
if(length(mt.i) != 1){
stop("Found more than 1 match")
}
if(sum(which(! is(test_recoded_model_data[,mt.i]) %in% is(train_model_data[,i]))) != 0){
if(length(is(test_recoded_model_data[,mt.i])) != length(is(train_model_data[,i]))){
if(is(test_recoded_model_data[,mt.i])[1] == "integer" & is(train_model_data[,mt.i])[1] == "numeric"){
next
}else{
if(is(test_recoded_model_data[,mt.i])[1] == "factor" & is(train_model_data[,mt.i])[1] == "logical"){
train_model_data[,mt.i] <- as.factor(as.character(train_model_data[,mt.i]))
}else{
print(i)
stop("Different classes for columns")
}
}
}
}
if(is.factor(train_model_data[,i]) == TRUE){
if(length(which(! levels(train_model_data[,i]) %in% levels(test_recoded_model_data[,mt.i]))) > 0){
if(length(levels(test_recoded_model_data[,mt.i])) < length(levels(train_model_data[,i])) & length(which(! levels(test_recoded_model_data[,mt.i]) %in% levels(train_model_data[,i]))) == 0){
new_dat.i <- as.factor(c(as.character(train_model_data[,i]), as.character(test_recoded_model_data[,i])))
train_model_data[,i] <- new_dat.i[1:nrow(train_model_data)]
test_recoded_model_data[,i] <- new_dat.i[(nrow(train_model_data)+1):length(new_dat.i)]
}else{
print(i)
stop("Factor levels not aligned")
}
}
}
}
#The above should run without errors or warnings
#################
#Train the trees#
#################
conditions <- colnames(train[[training_set]])[grep("severity", colnames(train[[training_set]]))]
results <- matrix(NA, nrow = length(conditions), ncol = 4)
colnames(results) <- c("raw_training", "raw_testing", "combined_mod_sev_training", "combined_mod_sev_testing")
results <- as.data.frame(results)
table_results_training <- list()
table_results_testing <- list()
big_results <- matrix(NA, ncol = (length(conditions) + 1), nrow = nrow(test_recoded_model_data))
colnames(big_results) <- c("ID", conditions)
big_results <- as.data.frame(big_results)
big_results$ID <- as.character(test_recoded$code)
trees <- list()
pb <- txtProgressBar(1, length(conditions), style=3)
for(i in 1:length(conditions)){
#Get the condition results for training and testing
train_condition.i <- train[[training_set]][,conditions[i]]
test_condition.i <- test_recoded[,conditions[i]]
if(method == "rpart"){
#I. Std. Decision Tree
#1. learn the tree
fit_tree_condition <- rpart(train_condition.i~.,data = train_model_data, method = "class", control=rpart.control(cp=0.0001, xval = 10, minsplit = 6))
#cp (cost complexity factor): each split must decrease the lack of fit by a factor of 0.001
#xval is the number of internal cross-validations
#minsplit is the min. number of obs. in a node before splitting is tried.
#this results in an overfitted tree so we must prune...
#want to select a tree size the minimizes the cross-valitrained error, the following does this by
#selecting a cp parameter that corresponds with the lowest xerror in     printcp()
#2. prune the tree
pfit_condition <- prune(fit_tree_condition, cp=fit_tree_condition$cptable[which.min(fit_tree_condition$cptable[,"xerror"]),"CP"])
}else{
if(method == "randomForest"){
#II. Random Forest
fit_tree_condition <- randomForest(train_condition.i~.,data = train_model_data, importance = TRUE, ntree = 500)
pfit_condition <- fit_tree_condition #this is just lazy book keeping so you can switch easily between rpart and randomForestv
}else{
stop("Method not implemented")
}
}
########################
#Predicting and scoring#
########################
prediction_in_sample <- predict(pfit_condition, train_model_data, type = 'class')
prediction_test <- predict(pfit_condition, test_recoded_model_data, type = 'class')
#raw score
raw_score_test <- length(which(as.character(prediction_test)==as.character(test_condition.i)))/length(prediction_test)
raw_score_train <- length(which(as.character(prediction_in_sample)==as.character(train_condition.i)))/length(prediction_in_sample)
#combined mod/sev score
mod_prediction_test <- as.character(prediction_test)
mod_test_condition.i <- as.character(test_condition.i)
mod_train_condition.i <- as.character(train_condition.i)
mod_prediction_in_sample <- as.character(prediction_in_sample)
mod_prediction_test[which(mod_prediction_test == "moderate" | mod_prediction_test == "severe")] <- "mod_sev"
mod_test_condition.i[which(mod_test_condition.i == "moderate" | mod_test_condition.i == "severe")] <- "mod_sev"
mod_train_condition.i[which(mod_train_condition.i == "moderate" | mod_train_condition.i == "severe")] <- "mod_sev"
mod_prediction_in_sample[which(mod_prediction_in_sample == "moderate" | mod_prediction_in_sample == "severe")] <- "mod_sev"
combined_mod_sev_training_score <- length(which(mod_prediction_test==mod_test_condition.i))/length(mod_prediction_test)
combined_mod_sev_testing_score <- length(which(mod_prediction_in_sample==mod_train_condition.i))/length(mod_prediction_in_sample)
#table score
tab_score_test <- table(prediction_test, test_condition.i)
tab_score_train <- table(prediction_in_sample, train_condition.i)
#encounter-level results
encounter_res.i <- rep(0, length(test_condition.i))
encounter_res.i[which(as.character(prediction_test)==as.character(test_condition.i))] <- 1
#plot of variable importance
if(do_plots == TRUE){
par(mar = c(12, 4, 2, 2))
barplot(pfit_condition$variable.importance/sum(pfit_condition$variable.importance), las = 2, cex.names = 0.8, ylab = "Variable importance")
}
#storing results
results$raw_training[i] <- raw_score_train
results$raw_testing[i] <- raw_score_test
results$combined_mod_sev_training[i] <- combined_mod_sev_training_score
results$combined_mod_sev_testing[i] <- combined_mod_sev_testing_score
table_results_training[[conditions[i]]] <- tab_score_train
table_results_testing[[conditions[i]]] <- tab_score_test
big_results[,conditions[i]] <- encounter_res.i
trees[[conditions[i]]] <- json_prsr(pfit_condition)
setTxtProgressBar(pb, i)
}
plot_var <- "combined_mod_sev_testing" #change to combined_mod_sev_testing to see combined or raw_testing for uncombinded
ord <- order(results[,plot_var], decreasing = TRUE)
names <- unlist(lapply(conditions, function(x) return(unlist(strsplit(x[1], "[.]"))[2])))
par(mar = c(7, 4, 3, 2))
barplot(results[ord, plot_var], names = names[ord], las = 2, ylab = "Out-of-sample accuracy", main = "Test case evaluation")
abline(h = 0.95, col = "red", lty = 3)
out <- data.frame(names[ord], results[ord, plot_var])
colnames(out) <- c("condition", "accuracy")
#save trees
if(do_save_trees == TRUE){
for(i in 1:length(trees)){
write_json(trees[[i]], path = paste0("Current Trees/",names(trees)[i], "_", time_stamp, ".json"))
}
}
ls()
setwd("~/Documents/ThinkMD/medsinc-ai")
setwd("~/Desktop/bank-additional")
system("ls")
dat <- read.csv("bank-additional-full.csv")
dim(dat)
7/60
1-(7/60)
7/67
-17/67
1-(7/67)
22/(45+22)
45/(45+22)
q()
load("/Users/scarpino/Desktop/ecap_individuallvl_23Aug17.RData")
count_miss <- function(x){
return(length(which(is.na(x) == TRUE)))
}
count_unique <- function(x){
return(length(unique(x)))
}
transform_NA <- function(x){
x_out <- rep(1, length(x))
x_out[which(is.na(x) == TRUE)] <- 0
return(x_out)
}
dat <- indiv
for(i in 1:ncol(dat)){
dat[,i] <- transform_NA(dat[,i])
}
cor_dat <- cor(dat)
for(i in 1:col(cor_dat)){
cor_dat[which(is.na(cor_dat[,i]) == TRUE),i] <- 0
}
test <- apply(indiv, 2, count_miss)
test2 <- apply(indiv, 2, count_unique)
test3 <- 1:ncol(indiv)
test_eval <- log(test+nrow(indiv)*0.01) * log(test3+1)
test_eval[order(test_eval)][1:20]
which(is.na(cor_dat[,i]) == TRUE)
cor_dat[,i]
library(pheatmap)
pheatmap(cor_dat)
for(i in 1:col(cor_dat)){
cor_dat[which(is.na(cor_dat[,i]) == TRUE),i] <- 0
}
for(i in 1:ncol(cor_dat)){
cor_dat[which(is.na(cor_dat[,i]) == TRUE),i] <- 0
}
pheatmap(cor_dat)
pheatmap(cor_dat, annotation_names_row = FALSE, annotation_names_col = FALSE)
?pheatmap
pheatmap(cor_dat, show_rownames = FALSE, show_colnames = FALSE)
test_eval <- log(test+nrow(indiv)*0.01)
test_eval[order(test_eval)][1:20]
cor(c(0,0,0,0), c(0,0,0,0))
?cor
cov(c(0,0,0,0), c(0,0,0,0))
cov(c(0,0,0,0), c(1,0,0,0))
cov(c(0,0,0,0), c(1,0,0,2))
cov(c(0,0,0,0), c(1,0,0,2))
test_eval <- test+2 * 1/test2 * test3+1
test_eval[order(test_eval)][1:20]
test_eval <- test+1 * 1/test2 * test3+1
test_eval[order(test_eval)][1:20]
test_eval <- test + 1/test2 + test3+1
test_eval[order(test_eval)][1:20]
test_eval <- test + 1/test2 + test3
test_eval[order(test_eval)][1:20]
test_eval <- test + 1/test2
test_eval[order(test_eval)][1:20]
plot(test_eval[order(test_eval)])
plot(test_eval[order(test_eval)][1:20])
plot(test_eval[order(test_eval)][1:10])
plot(log(test_eval[order(test_eval)][1:10]))
plot(log(test_eval[order(test_eval)]))
plot(log(1:length(test_eval)),log(test_eval[order(test_eval)]))
test_eval <- test + 1/test2 + test3
test_eval[order(test_eval)][1:20]
test_eval <- test + 1/test2
test_eval[order(test_eval)][1:20]
length(unique(indiv$fact_3_id))
nrow(indiv)
mean(test)
range(test)
hist(Test)
hist(test)
mean(test/nrow(indiv))
dbinom(c(0.025,0.975), size = 1, prob = 0.732)
dbinom(c(0.025,0.975), size = 1, prob = 0.732)
?dbinom
dbinom(c(2,0.97), size = 1, prob = 0.732)
dbinom(c(2,97), size = 1, prob = 0.732)
dbinom((50,97), size = 1, prob = 0.732)
dbinom(c(50,97), size = 1, prob = 0.732)
qbinom(p = c(0.025,0.975), size = 1, prob = 0.732)
qnbinom(p = c(0.025,0.975), size = 1, prob = 0.732)
qnbinom(p = c(0.025,0.975), size = 10, prob = 0.732)
qnbinom(p = c(0.025,0.975), size = ncol(indiv), prob = 0.732)
pnbinom(p = c(0.025,0.975), size = ncol(indiv), prob = 0.732)
pnbinom(q = c(0.025,0.975), size = ncol(indiv), prob = 0.732)
pnbinom(q = c(0.025,0.975), size = test[100], prob = 0.732)
test[100]
pnbinom(p = c(0.025,0.975), size = nro1(indiv), prob = 0.732)
pnbinom(p = c(0.025,0.975), size = nrow(indiv), prob = 0.732)
qnbinom(p = c(0.025,0.975), size = nrow(indiv), prob = 0.732)
length(which(test > 10100 & test < 10650))
length(which(test > 10100))
rm <- which(test == 0)
head(test[-rm][order(test[-rm])])
head(test[rm][order(test[rm])])
test[rm][order(test[rm])]
names(test[rm][order(test[rm])])
head(test[-rm][order(test[-rm])])
which(is.na(indiv$gender) == TRUE)
indiv[303,]
library(rjson)
islands[1:4]
toJSON(islands[1:4])
json_cars <- toJSON(as.list(cars))
json_cars
head(test[-rm][order(test[-rm])])
15/nrow(indiv)
nrow(indiv)
table(indiv$consent)
use <- which(indiv$consent == "useNot home/vacant")
use <- which(indiv$consent == "Not home/vacant")
use
use <- which(indiv$consent == "Not home/vacant" & is.na(indiv$gender) == TRUE)
use
use <- which(indiv$consent != "Not home/vacant" & is.na(indiv$gender) == TRUE)
use
indiv[use,]
q()
length(47:104)
104-47
58*30
length(108:124)
length(10:141)
length(110:141)
32+17
49*18
1300+800
2100/2500
1200/2100
q()
install.packages(survey)
install.packages('survey')
library(survey)
data(api)
dclus1<-svydesign(id=~dnum, fpc=~fpc, data=apiclus1)
dclus1
id
head(apiclus1)
data(api)
dclus1<-svydesign(id=~dnum, fpc=~acs.core, data=apiclus1)
data(api)
dclus1<-svydesign(id=~dnum, fpc=~fpc, data=apiclus1)
svyciprop(~I(ell==0), dclus1, method="li",level = .95)
svyciprop(~I(ell==0), dclus1, method="li",level = .99)
fit99 = svyciprop(~I(ell==0), dclus1, method="li",level = .99)
r.confint(fit99)
confint(fit99)
data(api)
dclus1<-svymean(id=~dnum, fpc=~fpc, data=apiclus1)
fit99 = svyciprop(~I(ell==0), dclus1, method="li",level = .99)
data(api)
dclus1<-svydesign(id=~dnum, fpc=~acs.core, data=apiclus1)
data(api)
dclus1<-svydesign(id=~dnum, fpc=~acs.core, data=apiclus1)
data(api)
dclus1<-svydesign(id=~dnum, fpc=~na.omit(acs.core), data=apiclus1)
data(api)
dclus1<-svydesign(id=~dnum, fpc=~acs.core, data=apiclus1)
data(api)
dclus1<-svydesign(id=~dnum, fpc=~acs.core, data=apiclus1, na.rm = TRUE)
data(api)
dclus1<-svydesign(id=~dnum, fpc=~acs.core, data=apiclus1, na.omiy = TRUE)
data(api)
dclus1<-svydesign(id=~dnum, fpc=~acs.core, data=apiclus1, na.omit = TRUE)
?svydesign
data(api)
dclus1<-svydesign(ids=~dnum, variables=~acs.core, data=apiclus1, na.omit = TRUE)
data(api)
dclus1<-svydesign(ids=~dnum, variables=~acs.core, data=apiclus1)
fit99 = svyciprop(~I(ell==0), dclus1, method="li",level = .99)
data(api)
dclus1<-svydesign(ids=~dnum, variables=~acs.core, data=apiclus1, na.action = "na.omit")
fit99 = svyciprop(~I(ell==0), dclus1, method="li",level = .99)
fit99 = svyciprop(~I(ell==0), dclus1, method="li",level = .99, na.rm = TRUE)
dclus1
data(api)
dclus1<-svydesign(id=~dnum, variables=~fpc, data=apiclus1)
fit99 = svyciprop(~I(ell==0), dclus1, method="li",level = .99, na.rm = TRUE)
dclus1<-svydesign(ids=~1, variables=fpc, data=apiclus1)
dclus1<-svydesign(ids=~1, variables=apiclus1$fpc, data=apiclus1)
dclus1<-svydesign(ids=~1, variables=apiclus1$fpc, data=apiclus1)
head(apiclus1)
dclus1<-svydesign(ids=~1, variables=apiclus1[,"fpc","acs.core"], data=apiclus1)
dclus1<-svydesign(ids=~1, variables=apiclus1[,"fpc","api.stu"], data=apiclus1)
Formula('~ 1', environment=env)
??Formula
library( Formula('~ 1', environment=env))
library(Formula)
??Formula
Formula('~ 1', environment=env)
Formula('~ 1')
?Formula
formula('~ 1')
dclus1<-svydesign(ids=~1, variables=list("fpc","api.stu"), data=apiclus1)
fit99 = svyciprop(~I(ell==0), dclus1, method="li",level = .99, na.rm = TRUE)
dclus1
fit99 = svyciprop(~I, dclus1, method="li",level = .99, na.rm = TRUE)
?svydesign
?svyciprop
?I
names(svydesign)
is(dclus1)
names(dclus1)
I(ell==0)
table(apiclus1$fpc)
apiclus1$fpc
table(apiclus1$dnum)
dclus1<-svydesign(id=~dnum, fpc=~fpc, data=apiclus1)
dclus1
dclus1$cluster
dclus1$strata
dclus1$ell
?svydesign
head(apistrat)
dclus1<-svydesign(id=~1, variables=list("sch.wide"), data=apiclus1)
svyciprop(~I, dclus1, method="li",level = .99, na.rm = TRUE)
svyciprop(~(~I(ell==0), dclus1, method="li",level = .99, na.rm = TRUE)
svyciprop(~I(ell==0), dclus1, method="li",level = .99, na.rm = TRUE)
talbe(apiclus1$dnum)
table(apiclus1$dnum)
apiclus1$dnum
dclus1<-svydesign(id=~dnum, fpc=~fpc, data=apiclus1)
dclus1$pps
dclus1$allprob
dclus1$ell
svyciprop(~I(ell==0), dclus1, method="li",level = .99, na.rm = TRUE)
dclus1<-svydesign(id=~1, variables=list("sch.wide"), data=apiclus1)
svyciprop(~I(ell==0), dclus1, method="li",level = .99, na.rm = TRUE)
svyciprop(~I(emer==0), dclus1, method="li",level = .99, na.rm = TRUE)
svyciprop(~I(), dclus1, method="li",level = .99, na.rm = TRUE)
svyciprop(~1, dclus1, method="li",level = .99, na.rm = TRUE)
HEAD(apiclus1)
head(apiclus1)
dclus1<-svydesign(id=~1, variables=list(colnames(apiclus1)), data=apiclus1)
svyciprop(~I(ell=0), dclus1, method="li",level = .99, na.rm = TRUE)
svyciprop(~I(ell=1), dclus1, method="li",level = .99, na.rm = TRUE)
dclus1<-svydesign(id=~1, variables=list("ell"), data=apiclus1)
svyciprop(~I(ell=1), dclus1, method="li",level = .99, na.rm = TRUE)
I(apiclus1$ell==1)
dclus1<-svydesign(id=~1, variables=list("ell","fpc"), data=apiclus1)
svyciprop(~I(ell=1), dclus1, method="li",level = .99, na.rm = TRUE)
svyciprop(~I(ell=1), dclus1, method="li",level = .99)
svyciprop(~I(ell=1), dclus1, method="li",level = .99)dclus1
dclus1
names(dclus1)
dclus1$variables
dclus1<-svydesign(id=~dnum, fpc=~fpc, data=apiclus1)
dclus1$variables
dclus1<-svydesign(id=~1, fpc=~fpc, data=apiclus1)
svyciprop(~I(ell=1), dclus1, method="li",level = .99)
svyciprop(~I(ell==1), dclus1, method="li",level = .99)
dclus1<-svydesign(id=~1, data=apiclus1)
svyciprop(~I(ell==1), dclus1, method="li",level = .99)
svyciprop(~I(ell==1), dclus1, method="li",level = .99)
head(dclus1)
head(apiclus1)
svyciprop(~I(acs.core==27), dclus1, method="li",level = .99)
dclus1<-svydesign(ids=~1, data=apiclus1)
svyciprop(~I(acs.core==27), dclus1, method="li",level = .99)
dclus1<-svydesign(ids=~1, variables = apiclus1, data=apiclus1)
svyciprop(~I(acs.core==27), dclus1, method="li",level = .99)
dclus1<-svydesign(ids=~1, variables = apiclus1[,c("acs.core")], data=apiclus1)
dclus1<-svydesign(ids=~1, variables = apiclus1[,c("acs.core","fpc")], data=apiclus1)
svyciprop(~I(acs.core==27), dclus1, method="li",level = .99)
svyciprop(~I(acs.core==27), dclus1, method="li",level = .99, na.rm = TRUE)
I(apiclus1$acs.core==27)
head(apiclus1)
dclus1<-svydesign(ids=~1, variables = apiclus1[,c("acs.core","acs.k3","fpc")], data=apiclus1)
svyciprop(~I(acs.core==27), dclus1, method="li",level = .99, na.rm = TRUE)
fit <- svymean(x=~I(acs.core==27), design = dclus1)
confint(fit)
fit <- svymean(x=~I(acs.core==27), design = dclus1, na.rm = TRUE)
confint(fit)
confint(fit)[1]
confint(fit)[2]
confint(fit)[3]
confint(fit)[4]
fit
?confint
confint(fit,"acs.core")
confint(fit,acs.core)
fit
confint(fit,"mean")
fit <- svymean(x=~I(acs.core==27), design = dclus1, na.rm = TRUE)
q()
q()
licence()
q()
library(swirl)
swirl()
install_course_github("swirldev", "R_Programming_E")
swirl()
q()
shiny::runApp('Desktop/moderator_buster/moderator_buster')
q()
shiny::runApp('Desktop/moderator_buster/moderator_buster')
q()
shiny::runApp('Desktop/moderator_buster/moderator_buster')
q()
shiny::runApp('Desktop/moderator_buster/moderator_buster')
q()
shiny::runApp('Desktop/moderator_buster/moderator_buster')
q()
shiny::runApp('Desktop/moderator_buster/moderator_buster')
q()
shiny::runApp('Desktop/moderator_buster/moderator_buster')
barplot(predictions[order(predictions, decreasing = TRUE)], names = allowed_words[order(predictions, decreasing = TRUE)])
predictions
redictions[order(predictions, decreasing = TRUE)]
predictions[order(predictions, decreasing = TRUE)]
allowed_words[order(predictions, decreasing = TRUE)]
barplot(predictions[order(predictions, decreasing = TRUE)], names = possible_categories[order(predictions, decreasing = TRUE)])
q()
shiny::runApp('Desktop/moderator_buster/moderator_buster')
q()
shiny::runApp('Desktop/moderator_buster/moderator_buster')
?textInput
q()
shiny::runApp('Desktop/moderator_buster/moderator_buster')
q()
shiny::runApp('Desktop/moderator_buster/moderator_buster')
q()
shiny::runApp('Desktop/moderator_buster/moderator_buster')
load("results_OOS.RData")
setwd("~/Desktop/moderator_buster/moderator_buster")
load("results_OOS.RData")
ls()
q()
